# DQN Training Evaluation Guide: Plots and Diagnostics
This document summarizes how to interpret the plots generated by the result_graph.py script during the TurtleBot3 training and how to diagnose common problems.

## 1. Key Metrics
The graph displays two distinct curves that must be analyzed together to understand the robot's "state of mind."

### ðŸ”´ Red Line: Total Reward (Reality)
- What it represents: The total sum of points actually obtained by the robot at the end of an episode.
- Meaning: Indicates how well the robot performed in the physical world (did it reach the goal? did it crash?).
- Data Source: cumulated_reward in start_deepqlearning.py.
### ðŸŸ¢ Green Line: Avg Max Q-Value (Expectation)
- What it represents: The average of the maximum Q-values predicted by the neural network for a batch of recent states.
- Meaning: Indicates how much reward the robot thinks it can obtain in the future. It represents its "confidence" or "self-esteem."
- Data Source: avg_max_q calculated via policy_net.


## 2. Common Scenarios and Diagnostics
Below are the 4 typical scenarios you might observe during training.
### âœ… Scenario A: Healthy Learning (Success)
- Plot:
    - Q-Value (Green): Starts low, rises gradually, and stabilizes at a high value (e.g., near 200-300).
    - Reward (Red): Fluctuates a lot initially, but the general trend rises and "chases" the green line.
- Interpretation: The robot is learning correctly. Its predictions (Green) match reality (Red).
- Action: None. Let the training continue.

### ðŸ’€ Scenario B: The "Flatline" (No Learning)
- Plot: Both lines remain stuck near zero or at very low values for hundreds of episodes.
- Interpretation: The robot never found the goal or stopped looking for it too early.
- Possible Causes:
    - Epsilon Decay too fast: The robot stops exploring before finding the goal for the first time (epsilon_decay parameter in stage1_params.yaml is too low).
    - Sparse Rewards: The goal is too difficult to reach by chance.
- Solution: Increase epsilon_decay (e.g., to 50,000 or more) or make the environment easier (move the goal closer).

### ðŸ’¥ Scenario C: The "Explosion" (Instability)
- Plot: The green line (Q-Values) shoots upwards or downwards with enormous values (e.g., $10^5$, $NaN$).
- Interpretation: The neural network is diverging; mathematical weights are becoming infinite.
- Possible Causes:
    - Learning Rate too high: The network makes "steps" that are too large when updating weights.
    - Reward Loop: You provided infinite positive rewards without a termination condition (e.g., positive reward for standing still).
- Solution:
    - Decrease learning_rate (e.g., from 0.001 to 0.0001).
    - Check gradient clipping in the code (present in start_deepqlearning.py: param.grad.data.clamp_(-1, 1)).

### ðŸ¤¥ Scenario D: The "Disappointment" (Overestimation)
- Plot:
    - Q-Value (Green): Very high (e.g., predicts 300 points).
    - Reward (Red): Low (e.g., gets -100 points).
- Interpretation: The robot is "arrogant." It believes it knows what to do, but consistently fails. This phenomenon is known as Maximization Bias.
- Possible Causes:
    - Incorrect Target Network Update: The "target" network changes too frequently, making learning unstable.
- Solution: Increase the target_update interval and ensure it is based on episodes and not steps (as corrected in your code).

## 3. Key Parameters to Tune
If the plots are not satisfactory, these are the parameters in the stage1_params.yaml file to adjust:
| Parameter      | Effect on Plot | Recommended Value      |
|----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|
| `gamma`   | Determines how far into the future the robot looks. If low (0.9), Q-values (Green) remain low. If high (0.99), Q-values rise because they include distant rewards. | 0.99                  |
| `epsilon_decay`| Controls the duration of the initial "chaotic" phase. If the plot is flat (Scenario B), INCREASE IT. | 50,000+  |
| `target_update`| Stabilizes the Green line. If Green and Red diverge too much (Scenario D), adjust this value (using episode-based logic). | 10â€“20 (episodes)      |
| `learning_rate`| The speed of learning. If the Green line explodes (Scenario C), DECREASE IT. | 0.00025   |

4. Note on the "Statue Robot" (Reward Hacking)
A special case not always visible from Q-Value plots, but visible by observing the robot:
- Symptom: The robot stops and spins in place indefinitely.
- Plot: Reward (Red) is constantly positive (but not maximum), Q-Value is stable.
- Cause: The reward function gives points for orientation (yaw_reward) even if the robot is stationary. The robot discovered that spinning is safer than moving.
- Solution: Ensure that the sum of yaw_reward + time_penalty is negative if the robot does not move forward, forcing it to move to gain points (as corrected in your _compute_reward function).